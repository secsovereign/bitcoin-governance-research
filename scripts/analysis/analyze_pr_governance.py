#!/usr/bin/env python3
"""
AI-Powered Governance Analysis for Specific PRs

This script takes a PR context file (generated by gather_pr_context.py) and
provides structured prompts and analysis framework for AI agents to analyze
governance situations, failures, or patterns.

Usage:
    python analyze_pr_governance.py pr_context_12345.json --output analysis_12345.md
"""

import sys
import json
import argparse
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

logger = setup_logger()


class PRGovernanceAnalyzer:
    """Generate AI analysis prompts and framework for PR governance analysis."""
    
    def __init__(self, context: Dict[str, Any]):
        """Initialize analyzer with PR context."""
        self.context = context
        self.pr = context.get('github_pr', {})
        self.pr_number = context.get('pr_number')
    
    def generate_analysis_prompt(self) -> str:
        """Generate comprehensive analysis prompt for AI agent."""
        prompt = f"""# Governance Analysis: PR #{self.pr_number}

## PR Overview
- **Title**: {self.pr.get('title', 'N/A')}
- **Author**: {self.pr.get('user', {}).get('login', 'N/A')}
- **Created**: {self.pr.get('created_at', 'N/A')}
- **Merged**: {self.pr.get('merged', False)}
- **Merged By**: {self.pr.get('merged_by', {}).get('login', 'N/A') if self.pr.get('merged_by') else 'N/A'}
- **State**: {self.pr.get('state', 'N/A')}
- **Labels**: {', '.join([l.get('name') if isinstance(l, dict) else str(l) for l in self.pr.get('labels', [])]) or 'None'}

## Governance Indicators

### Self-Merge
{'⚠️ **SELF-MERGE DETECTED**: Author merged their own PR' if self.context.get('governance_indicators', {}).get('self_merge') else '✅ Not a self-merge'}

### Review Status
{'⚠️ **ZERO REVIEWS**: No reviews found' if self.context.get('governance_indicators', {}).get('zero_reviews') else f"✅ Reviews: {len(self.pr.get('reviews', []))}"}

### Maintainer Involvement
"""
        
        maintainers = self.context.get('governance_indicators', {}).get('maintainer_involvement', [])
        if maintainers:
            prompt += f"- **{len(maintainers)} maintainer(s) involved**:\n"
            for m in maintainers:
                prompt += f"  - {m.get('username')} ({', '.join(m.get('roles', []))})\n"
        else:
            prompt += "- No maintainers directly involved\n"
        
        prompt += f"""
### Conflict Indicators
"""
        conflicts = self.context.get('governance_indicators', {}).get('conflict_indicators', [])
        if conflicts:
            prompt += f"- **{len(conflicts)} conflict indicator(s) found**:\n"
            for conflict in conflicts[:5]:  # Top 5
                prompt += f"  - {conflict.get('type')} by {conflict.get('author')} at {conflict.get('timestamp', '')[:10]}\n"
                prompt += f"    Excerpt: {conflict.get('excerpt', '')[:150]}...\n"
        else:
            prompt += "- No conflict indicators detected\n"
        
        prompt += f"""
### Transparency Indicators
"""
        transparency = self.context.get('governance_indicators', {}).get('transparency_indicators', [])
        if transparency:
            for indicator in transparency:
                if indicator.get('type') == 'cross_platform_discussion':
                    prompt += f"- **Cross-platform discussion**:\n"
                    prompt += f"  - IRC messages: {indicator.get('irc_messages', 0)}\n"
                    prompt += f"  - Mailing list emails: {indicator.get('emails', 0)}\n"
        else:
            prompt += "- No cross-platform discussion detected\n"
        
        prompt += f"""
## Timeline of Events

"""
        timeline = self.context.get('timeline', [])
        for event in timeline[:20]:  # First 20 events
            prompt += f"- **{event.get('timestamp', '')[:19]}** [{event.get('type', 'unknown')}]: {event.get('actor', 'unknown')} - {event.get('description', '')[:80]}\n"
        
        if len(timeline) > 20:
            prompt += f"\n... and {len(timeline) - 20} more events\n"
        
        prompt += f"""
## Participants

"""
        participants = self.context.get('participants', {})
        for username, data in list(participants.items())[:10]:  # Top 10
            maintainer_badge = " [MAINTAINER]" if data.get('is_maintainer') else ""
            prompt += f"- **{username}**{maintainer_badge}:\n"
            prompt += f"  - Roles: {', '.join(data.get('roles', []))}\n"
            prompt += f"  - Activity count: {data.get('activity_count', 0)}\n"
        
        if len(participants) > 10:
            prompt += f"\n... and {len(participants) - 10} more participants\n"
        
        prompt += f"""
## Critical Analysis Requirements

### Deductions vs Inferences

**You MUST explicitly differentiate between deductions and inferences in your analysis:**

- **[DEDUCTION]**: A conclusion that follows necessarily from the data. Facts that can be directly observed.
  - Example: "Commit 0bf7b38b on 2020-07-13 contains `fs::remove_all()`" (verifiable in commit diff)
  - Example: "PR has 98 reviews" (countable from data)
  - Example: "Sjors left a review comment on 2022-08-26" (directly observable)
  - **Format**: State as fact: "The data shows..." or "Commit X contains..." or "[DEDUCTION]..."

- **[INFERENCE]**: A conclusion drawn from available evidence but not directly observable.
  - Example: "The comment 'Latest push adds cleanup' likely refers to cleanup code" (inference from comment)
  - Example: "The cleanup code was probably added late" (inference, not directly observable)
  - Example: "This suggests the code wasn't reviewed" (inference from absence of evidence)
  - **Format**: Explicitly state as inference: "Based on [evidence], I infer..." or "[INFERENCE]..." or "This suggests..."

**When making ANY claim, you MUST label it as either [DEDUCTION] or [INFERENCE].**

### Commit History Analysis

**You MUST unwind the complete commit history:**

1. **Get ALL commits in the PR** - Use GitHub API: `GET /repos/bitcoin/bitcoin/pulls/{PR_NUMBER}/commits`
2. **Trace code changes to specific commits** - For any code pattern mentioned, identify the exact commit
3. **Check commit diffs** - Don't infer from comments, verify in commit diffs
4. **Build complete timeline** - When was each piece of code added/modified/removed?

**Don't make unnecessary inferences** - If you can see the commit, state it as a deduction. If you can't see the commit but have a comment, state it as an inference.

## Analysis Questions

Please analyze this PR's governance situation and answer the following questions. **Label every claim as [DEDUCTION] or [INFERENCE].**

### 1. Decision-Making Process
- How was the decision to merge/reject made? [Label each claim]
- Was there adequate review and discussion? [Label each claim]
- Were decision criteria clear and consistently applied? [Label each claim]

### 2. Power Dynamics
- Who had influence over this PR's outcome? [Label each claim]
- Were maintainers involved appropriately? [Label each claim]
- Was there any evidence of power concentration or abuse? [Label each claim]

### 3. Transparency
- Was discussion transparent and public? [Label each claim]
- Were concerns raised and addressed? [Label each claim]
- Was there cross-platform coordination (IRC, mailing lists)? [Label each claim]

### 4. Governance Failures (if any)
- Were there any governance failures or red flags? [Label each claim]
- Examples: self-merge, zero reviews, lack of transparency, conflicts ignored
- What could have been done better? [Label each claim]

### 5. Patterns
- Does this PR exhibit patterns seen in other PRs? [Label each claim]
- Is this typical or atypical for Bitcoin Core governance? [Label each claim]
- What does this tell us about the governance system? [Label each claim]

### 6. Specific People/Relationships
- Analyze relationships between participants [Label each claim]
- Were there conflicts or tensions? [Label each claim]
- Did personal relationships influence the outcome? [Label each claim]

### 7. Code Change Analysis
- **Trace every significant code change to its specific commit** [DEDUCTION required]
- When was each piece of code added? [Check commit diffs, don't infer]
- Which commits contain which changes? [Verify in diffs]
- What code was reviewed vs. not reviewed? [Label as DEDUCTION if verifiable, INFERENCE if not]

## Data Sources Available

- **GitHub PR**: Full PR data with comments and reviews
- **IRC Messages**: {len(self.context.get('irc_messages', []))} messages
- **Mailing List Emails**: {len(self.context.get('mailing_list_emails', []))} emails
- **Related Commits**: {len(self.context.get('related_commits', []))} commits
- **Timeline**: {len(timeline)} events total

**IMPORTANT**: If the commit count seems low, you MUST fetch ALL commits in the PR using GitHub API:
```bash
curl "https://api.github.com/repos/bitcoin/bitcoin/pulls/{self.pr_number}/commits"
```

This will give you every commit that's part of the PR, not just merge commits or commits by author.

## Full Context Available

The full context JSON file contains:
- Complete PR data (comments, reviews, metadata)
- All IRC messages mentioning this PR
- All mailing list emails mentioning this PR
- Related commits
- Complete timeline of events
- Participant analysis
- Governance indicators

Use this data to provide a comprehensive governance analysis.
"""
        
        return prompt
    
    def generate_structured_analysis(self) -> Dict[str, Any]:
        """Generate structured analysis framework."""
        analysis = {
            'pr_number': self.pr_number,
            'analysis_date': datetime.now().isoformat(),
            'summary': {
                'title': self.pr.get('title', ''),
                'author': self.pr.get('user', {}).get('login', ''),
                'outcome': 'merged' if self.pr.get('merged') else 'closed' if self.pr.get('state') == 'closed' else 'open',
                'merged_by': self.pr.get('merged_by', {}).get('login') if self.pr.get('merged_by') else None,
            },
            'governance_indicators': self.context.get('governance_indicators', {}),
            'key_findings': [],
            'decision_making_analysis': {},
            'power_dynamics_analysis': {},
            'transparency_analysis': {},
            'governance_failures': [],
            'patterns_observed': [],
            'participant_analysis': {},
            'recommendations': []
        }
        
        return analysis
    
    def save_analysis(self, output_file: Path, format: str = 'markdown'):
        """Save analysis to file."""
        if format == 'markdown':
            prompt = self.generate_analysis_prompt()
            with open(output_file, 'w') as f:
                f.write(prompt)
            logger.info(f"Saved analysis prompt to {output_file}")
        elif format == 'json':
            analysis = self.generate_structured_analysis()
            with open(output_file, 'w') as f:
                json.dump(analysis, f, indent=2)
            logger.info(f"Saved structured analysis to {output_file}")
        else:
            raise ValueError(f"Unknown format: {format}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Generate AI analysis prompts for PR governance analysis'
    )
    parser.add_argument(
        'context_file',
        type=Path,
        help='PR context JSON file (from gather_pr_context.py)'
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        default=None,
        help='Output file path (default: analysis_{pr_number}.md)'
    )
    parser.add_argument(
        '--format',
        choices=['markdown', 'json'],
        default='markdown',
        help='Output format (default: markdown)'
    )
    
    args = parser.parse_args()
    
    # Load context
    if not args.context_file.exists():
        logger.error(f"Context file not found: {args.context_file}")
        sys.exit(1)
    
    with open(args.context_file) as f:
        context = json.load(f)
    
    # Generate analysis
    analyzer = PRGovernanceAnalyzer(context)
    
    # Determine output file
    if args.output:
        output_file = args.output
    else:
        pr_number = context.get('pr_number', 'unknown')
        output_dir = project_root / 'data' / 'pr_analyses'
        output_dir.mkdir(parents=True, exist_ok=True)
        ext = 'md' if args.format == 'markdown' else 'json'
        output_file = output_dir / f"analysis_{pr_number}.{ext}"
    
    analyzer.save_analysis(output_file, format=args.format)
    
    print(f"\n✅ Analysis generated: {output_file}")
    print(f"\nYou can now use this file with an AI agent to analyze the governance situation.")


if __name__ == '__main__':
    main()
